{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d91176d-83bb-4e32-9fa2-8741ad9d247f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "new_path = 'third_part/soxan'\n",
    "sys.path.append(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26068aaa-bf93-41ef-8d14-2113b993a4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import Wav2Vec2ForSpeechClassification\n",
    "pretrain_model_path = 'soxan_checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e648c4e-0dc9-4abf-bcd7-de218d788f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15604,) torch.Size([1, 15604]) dict_keys(['input_values'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from transformers import AutoConfig, Wav2Vec2FeatureExtractor\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = AutoConfig.from_pretrained(pretrain_model_path)\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(pretrain_model_path)\n",
    "sampling_rate = feature_extractor.sampling_rate\n",
    "\n",
    "# for wav2vec\n",
    "model = Wav2Vec2ForSpeechClassification.from_pretrained(pretrain_model_path).to(device)\n",
    "\n",
    "\n",
    "\n",
    "def speech_file_to_array_fn(wave, sampling_rate):\n",
    "    # speech_array, _sampling_rate = torchaudio.load(path)\n",
    "    speech_array = wave\n",
    "    resampler = torchaudio.transforms.Resample(16800, sampling_rate)\n",
    "    speech = resampler(speech_array).squeeze().numpy()\n",
    "    return speech\n",
    "\n",
    "\n",
    "def predict(path, sampling_rate):\n",
    "    speech = speech_file_to_array_fn(path, sampling_rate)\n",
    "    inputs = feature_extractor(speech, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "    print(speech.shape, inputs['input_values'].shape, inputs.keys())\n",
    "    inputs = {key: inputs[key].to(device) for key in inputs}\n",
    "    with torch.no_grad():\n",
    "        features = model.extract_feature(**inputs)\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    scores = F.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
    "    outputs = [{\"Emotion\": config.id2label[i], \"Score\": f\"{round(score * 100, 3):.1f}%\"} for i, score in\n",
    "               enumerate(scores)]\n",
    "    return outputs, features\n",
    "\n",
    "\n",
    "path = \"data/Data/genres_original/blues/blues.00000.wav\"\n",
    "audio_input_16khz = torch.randn(1, 1024*16)\n",
    "outputs, features = predict(audio_input_16khz, sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98526b1e-9224-48a8-a194-9b00dc5dd9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
